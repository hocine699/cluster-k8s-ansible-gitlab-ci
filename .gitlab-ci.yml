# Pipeline GitLab CI pour déploiement cluster Kubernetes
# Compatible avec GitLab hébergé sur Synology NAS

stages:
  - cleanup
  - validate
  - prepare
  - deploy-cluster
  - verify
  - deploy-app
  - maintenance

variables:
  ANSIBLE_HOST_KEY_CHECKING: "False"
  ANSIBLE_FORCE_COLOR: "True"
  ANSIBLE_STDOUT_CALLBACK: "yaml"
  # Variables à définir dans GitLab CI/CD Variables
  # K8S_MASTER_IP, K8S_WORKER1_IP, K8S_WORKER2_IP
  # SSH_PRIVATE_KEY (base64 encoded)

# Template pour les jobs Ansible avec votre image personnalisée
.ansible_job: &ansible_job
  image: mon-runner-devops:latest
  tags:
    - synology
  before_script:
    # Vos dépendances sont déjà dans l'image mon-runner-devops
    - mkdir -p ~/.ssh
    # Utilisation directe de la clé SSH depuis GitLab Variables (pas de base64)
    - echo "$SSH_PRIVATE_KEY" > ~/.ssh/id_rsa
    - chmod 600 ~/.ssh/id_rsa
    - ssh-keyscan -H $K8S_MASTER_IP >> ~/.ssh/known_hosts
    - ssh-keyscan -H $K8S_WORKER1_IP >> ~/.ssh/known_hosts
    - ssh-keyscan -H $K8S_WORKER2_IP >> ~/.ssh/known_hosts
    # Générer l'inventaire dynamique avec les variables CI
    - |
      cat > inventory.yml << EOF
      all:
        children:
          masters:
            hosts:
              k8s-master-01:
                ansible_host: $K8S_MASTER_IP
                ansible_user: hocine
                ansible_ssh_private_key_file: ~/.ssh/id_rsa
                node_role: master
          workers:
            hosts:
              k8s-worker-01:
                ansible_host: $K8S_WORKER1_IP
                ansible_user: hocine
                ansible_ssh_private_key_file: ~/.ssh/id_rsa
                node_role: worker
              k8s-worker-02:
                ansible_host: $K8S_WORKER2_IP
                ansible_user: hocine
                ansible_ssh_private_key_file: ~/.ssh/id_rsa
                node_role: worker
        vars:
          ansible_python_interpreter: /usr/bin/python3
          ansible_ssh_common_args: '-o StrictHostKeyChecking=no'
      EOF

# Stage 0: Nettoyage préalable
cleanup_environment:
  stage: cleanup
  <<: *ansible_job
  script:
    - echo "=== NETTOYAGE COMPLET DU CLUSTER KUBERNETES ==="
    - |
      # Nettoyage complet de tous les composants Kubernetes
      ansible all -i inventory.yml -m shell -a "
        echo 'Début du nettoyage sur $(hostname)...'
        
        # Arrêt forcé de tous les processus Kubernetes
        sudo pkill -f kube-apiserver || true
        sudo pkill -f kube-controller-manager || true
        sudo pkill -f kube-scheduler || true
        sudo pkill -f kubelet || true
        sudo pkill -f kube-proxy || true
        sudo pkill -f etcd || true
        
        # Arrêt des services Kubernetes
        sudo systemctl stop kubelet || true
        sudo systemctl disable kubelet || true
        sudo systemctl stop docker || true
        sudo systemctl stop containerd || true
        
        # Attente que les processus s'arrêtent
        sleep 5
        
        # Vérification et kill forcé si nécessaire
        sudo fuser -k 6443/tcp || true
        sudo fuser -k 2379/tcp || true
        sudo fuser -k 2380/tcp || true
        sudo fuser -k 10250/tcp || true
        
        # Reset complet de kubeadm
        sudo kubeadm reset -f || true
        
        # Suppression des configurations Kubernetes
        sudo rm -rf /etc/kubernetes/ || true
        sudo rm -rf ~/.kube/ || true
        sudo rm -rf /var/lib/kubelet/ || true
        sudo rm -rf /var/lib/etcd/ || true
        sudo rm -rf /etc/cni/net.d/ || true
        sudo rm -rf /opt/cni/bin/ || true
        
        # Nettoyage des conteneurs et images
        sudo ctr -n k8s.io containers rm \$(sudo ctr -n k8s.io containers list -q) || true
        sudo ctr -n k8s.io images rm \$(sudo ctr -n k8s.io images list -q) || true
        sudo ctr namespaces rm k8s.io || true
        
        # Nettoyage des interfaces réseau
        sudo ip link delete cni0 || true
        sudo ip link delete flannel.1 || true
        sudo ip route flush table main || true
        
        # Nettoyage des repositories et clés APT
        sudo rm -rf /etc/apt/sources.list.d/kubernetes* || true
        sudo rm -rf /etc/apt/keyrings/kubernetes* || true
        sudo rm -rf /etc/apt/trusted.gpg.d/kubernetes* || true
        sudo rm -rf /usr/share/keyrings/kubernetes* || true
        
        # Nettoyage des iptables (attention!)
        sudo iptables -F || true
        sudo iptables -t nat -F || true
        sudo iptables -t mangle -F || true
        sudo iptables -X || true
        
        # Nettoyage APT
        sudo apt-get clean || true
        sudo rm -rf /var/lib/apt/lists/* || true
        
        # Redémarrage des services réseau
        sudo systemctl restart containerd || true
        sudo systemctl restart networkd-dispatcher || true
        
        echo 'Nettoyage terminé sur $(hostname)'
      "
    - echo "=== NETTOYAGE TERMINÉ SUR TOUTES LES VMs ==="
    - echo "Attente de stabilisation des services..."
    - sleep 10
    - echo "=== VÉRIFICATION POST-NETTOYAGE ==="
    - |
      ansible all -i inventory.yml -m shell -a "
        echo 'Vérification sur $(hostname):'
        echo '- Kubelet: '$(sudo systemctl is-active kubelet || echo 'arrêté')
        echo '- Containerd: '$(sudo systemctl is-active containerd || echo 'redémarré')
        echo '- Kubernetes configs: '$(ls /etc/kubernetes/ 2>/dev/null | wc -l)' fichiers'
        echo '- Conteneurs actifs: '$(sudo ctr -n k8s.io containers list 2>/dev/null | wc -l)' conteneurs'
        echo '- Port 6443: '$(sudo netstat -tulpn | grep :6443 || echo 'libre')
        echo '- Port 2379 (etcd): '$(sudo netstat -tulpn | grep :2379 || echo 'libre') 
        echo '- Processus kube: '$(ps aux | grep -E '[k]ube|[e]tcd' | wc -l)' processus'
        echo 'Prêt pour nouveau déploiement'
      " || true
    - echo "=== ENVIRONNEMENT NETTOYÉ ET PRÊT ==="
  allow_failure: true
  only:
    - main
    - develop

# Stage 1: Validation
validate_syntax:
  stage: validate
  <<: *ansible_job
  script:
    - ansible-playbook --syntax-check -i inventory.yml site.yml
    - ansible-lint site.yml || true  # Warning only
  dependencies:
    - cleanup_environment
  only:
    - merge_requests
    - main
    - develop

validate_inventory:
  stage: validate
  <<: *ansible_job
  script:
    - ansible all -i inventory.yml -m ping --timeout=10
  dependencies:
    - cleanup_environment
  only:
    - merge_requests
    - main
    - develop

# Stage 2: Préparation
prepare_environment:
  stage: prepare
  <<: *ansible_job
  script:
    - echo "Préparation de l'environnement..."
    - ansible all -i inventory.yml -m setup
    - ansible all -i inventory.yml -m shell -a "uptime"
  artifacts:
    reports:
      junit: test-results.xml
    expire_in: 1 hour
  only:
    - main
    - develop

# Stage 3: Déploiement du cluster
deploy_common:
  stage: deploy-cluster
  <<: *ansible_job
  script:
    - echo "Déploiement de la configuration commune..."
    - ansible-playbook -i inventory.yml site.yml --tags common -v
  dependencies:
    - prepare_environment
  artifacts:
    paths:
      - logs/
    expire_in: 1 day
  only:
    - main
    - develop

deploy_container_runtime:
  stage: deploy-cluster
  <<: *ansible_job
  script:
    - echo "Installation du runtime de conteneurs..."
    - ansible-playbook -i inventory.yml site.yml --tags containerd -v
  dependencies:
    - deploy_common
  artifacts:
    paths:
      - logs/
    expire_in: 1 day
  only:
    - main
    - develop

deploy_kubernetes:
  stage: deploy-cluster
  <<: *ansible_job
  script:
    - echo "Installation de Kubernetes..."
    - ansible-playbook -i inventory.yml site.yml --tags kubernetes -v
  dependencies:
    - deploy_container_runtime
  artifacts:
    paths:
      - logs/
    expire_in: 1 day
  only:
    - main
    - develop

deploy_master:
  stage: deploy-cluster
  <<: *ansible_job
  script:
    - echo "Configuration du nœud master..."
    - ansible-playbook -i inventory.yml site.yml --limit masters -v
  dependencies:
    - deploy_kubernetes
  artifacts:
    paths:
      - logs/
      - kubeconfig/
      - dashboard-info/
    expire_in: 1 week
  after_script:
    - mkdir -p kubeconfig dashboard-info logs
    # Diagnostic de base
    - |
      echo "=== DIAGNOSTIC DU CLUSTER ===" | tee logs/diagnostic.log
      ssh -o StrictHostKeyChecking=no hocine@$K8S_MASTER_IP "
        echo 'Statut des nœuds:' >> diagnostic.log
        kubectl get nodes -o wide >> diagnostic.log 2>&1 || echo 'Erreur kubectl nodes' >> diagnostic.log
        echo '' >> diagnostic.log
        echo 'Statut des pods système:' >> diagnostic.log  
        kubectl get pods -n kube-system >> diagnostic.log 2>&1 || echo 'Erreur kubectl pods system' >> diagnostic.log
        echo '' >> diagnostic.log
        echo 'Statut des pods dashboard:' >> diagnostic.log
        kubectl get pods -n kubernetes-dashboard >> diagnostic.log 2>&1 || echo 'Dashboard non installé' >> diagnostic.log
        echo '' >> diagnostic.log
        echo 'Services:' >> diagnostic.log
        kubectl get svc -A >> diagnostic.log 2>&1 || echo 'Erreur kubectl services' >> diagnostic.log
      "
    - scp -o StrictHostKeyChecking=no hocine@$K8S_MASTER_IP:diagnostic.log logs/ || echo "Impossible de récupérer les logs de diagnostic"
    # Récupération de la kubeconfig si elle existe
    - scp -o StrictHostKeyChecking=no hocine@$K8S_MASTER_IP:~/.kube/config kubeconfig/config || echo "Kubeconfig non trouvée"
    # Récupération des informations du dashboard si disponibles
    - |
      ssh -o StrictHostKeyChecking=no hocine@$K8S_MASTER_IP "
        if kubectl get namespace kubernetes-dashboard 2>/dev/null; then
          SECRET=\$(kubectl get secrets -n kubernetes-dashboard | grep admin-user | awk '{print \$1}' | head -1)
          if [ ! -z \"\$SECRET\" ]; then
            kubectl describe secret \$SECRET -n kubernetes-dashboard | grep token: | awk '{print \$2}' > dashboard-token.txt
            echo 'Dashboard URL: https://$K8S_MASTER_IP:30443' > dashboard-info.txt
            echo 'Token pour connexion:' >> dashboard-info.txt
            cat dashboard-token.txt >> dashboard-info.txt
          else
            echo 'Token admin non trouvé' > dashboard-info.txt
          fi
        else
          echo 'Namespace kubernetes-dashboard non trouvé' > dashboard-info.txt
        fi
      "
    - scp -o StrictHostKeyChecking=no hocine@$K8S_MASTER_IP:dashboard-* dashboard-info/ || echo "Pas d'informations dashboard disponibles"
  only:
    - main
    - develop

deploy_workers:
  stage: deploy-cluster
  <<: *ansible_job
  script:
    - echo "Configuration des nœuds workers..."
    - ansible-playbook -i inventory.yml site.yml --limit workers -v
  dependencies:
    - deploy_master
  artifacts:
    paths:
      - logs/
    expire_in: 1 day
  only:
    - main
    - develop

# Stage 4: Vérification
verify_cluster:
  stage: verify
  <<: *ansible_job
  script:
    - echo "Vérification du cluster..."
    - mkdir -p ~/.kube
    - scp -o StrictHostKeyChecking=no hocine@$K8S_MASTER_IP:~/.kube/config ~/.kube/config
    - kubectl get nodes -o wide
    - kubectl get pods -A
    - kubectl cluster-info
    - |
      # Test de déploiement simple
      kubectl create deployment nginx-test --image=nginx:latest || true
      kubectl expose deployment nginx-test --port=80 --type=NodePort || true
      sleep 30
      kubectl get pods -l app=nginx-test
      kubectl delete deployment nginx-test || true
      kubectl delete service nginx-test || true
  dependencies:
    - deploy_workers
  artifacts:
    reports:
      junit: cluster-test-results.xml
    expire_in: 1 week
  only:
    - main
    - develop

# Stage 5: Déploiement d'applications (optionnel)
# Stage 5: Déploiement d'applications et interfaces
deploy_dashboard:
  stage: deploy-app
  <<: *ansible_job
  script:
    - echo "Déploiement du Kubernetes Dashboard..."
    - mkdir -p ~/.kube
    - scp -o StrictHostKeyChecking=no hocine@$K8S_MASTER_IP:~/.kube/config ~/.kube/config
    - kubectl apply -f k8s-dashboard.yaml
    - sleep 60
    - kubectl get pods -n kubernetes-dashboard
    - kubectl get services -n kubernetes-dashboard
    - |
      echo "Récupération du token d'accès..."
      TOKEN=$(kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath='{.data.token}' | base64 -d)
      echo "Dashboard URL: https://$K8S_MASTER_IP:30443"
      echo "Token: $TOKEN"
      echo "Dashboard accessible à: https://$K8S_MASTER_IP:30443"
      echo "Token d'accès: $TOKEN" > dashboard-access.txt
  dependencies:
    - verify_cluster
  artifacts:
    paths:
      - dashboard-access.txt
    expire_in: 1 week
  only:
    - main

deploy_monitoring:
  stage: deploy-app
  <<: *ansible_job
  script:
    - echo "Déploiement du monitoring pour intégration NAS..."
    - mkdir -p ~/.kube monitoring-config
    - scp -o StrictHostKeyChecking=no hocine@$K8S_MASTER_IP:~/.kube/config ~/.kube/config
    - kubectl apply -f k8s-monitoring-stack.yaml
    - sleep 60
    - kubectl get pods -n kube-system -l app=node-exporter
    - kubectl get pods -n kube-system -l app=kube-state-metrics
    - |
      # Récupération du token Prometheus
      echo "Récupération du token d'authentification..."
      TOKEN=$(kubectl get secret prometheus-k8s-token -n kube-system -o jsonpath='{.data.token}' | base64 -d)
      echo "$TOKEN" > monitoring-config/prometheus-k8s-token.txt
      
      # Génération de la config Prometheus
      cat > monitoring-config/prometheus-k8s-jobs.yml << EOF
      # Configuration à ajouter à votre prometheus.yml sur le NAS
      
      scrape_configs:
        # Métriques des nœuds (Node Exporter)
        - job_name: 'kubernetes-nodes'
          static_configs:
            - targets:
              - '$K8S_MASTER_IP:9100'
              - '$K8S_WORKER1_IP:9100'
              - '$K8S_WORKER2_IP:9100'
          scrape_interval: 30s
        
        # Métriques Kubernetes (kube-state-metrics)
        - job_name: 'kube-state-metrics'
          static_configs:
            - targets:
              - '$K8S_MASTER_IP:30080'
          scrape_interval: 30s
        
        # API Server Kubernetes
        - job_name: 'kubernetes-apiserver'
          scheme: https
          tls_config:
            insecure_skip_verify: true
          bearer_token_file: /etc/prometheus/k8s-token.txt
          static_configs:
            - targets:
              - '$K8S_MASTER_IP:6443'
          scrape_interval: 30s
        
        # Kubelet cAdvisor
        - job_name: 'kubernetes-kubelet'
          scheme: https
          tls_config:
            insecure_skip_verify: true
          bearer_token_file: /etc/prometheus/k8s-token.txt
          static_configs:
            - targets:
              - '$K8S_MASTER_IP:10250'
              - '$K8S_WORKER1_IP:10250'
              - '$K8S_WORKER2_IP:10250'
          metrics_path: /metrics/cadvisor
          scrape_interval: 30s
      EOF
      
      # Informations Grafana
      cat > monitoring-config/grafana-dashboards.txt << EOF
      DASHBOARDS GRAFANA À IMPORTER:
      
      1. Kubernetes Cluster Monitoring (ID: 7249)
      2. Kubernetes Pod Monitoring (ID: 6417)
      3. Node Exporter Full (ID: 1860)
      4. Kubernetes Deployment (ID: 8588)
      
      ENDPOINTS MONITORING:
      - Node Exporter: $K8S_MASTER_IP:9100, $K8S_WORKER1_IP:9100, $K8S_WORKER2_IP:9100
      - kube-state-metrics: $K8S_MASTER_IP:30080
      - API Server: $K8S_MASTER_IP:6443
      - Dashboard K8s: https://$K8S_MASTER_IP:30443
      EOF
      
      echo "Configuration monitoring générée dans monitoring-config/"
  dependencies:
    - deploy_dashboard
  artifacts:
    paths:
      - monitoring-config/
    expire_in: 1 week
  only:
    - main

deploy_test_app:
  stage: deploy-app
  <<: *ansible_job
  script:
    - echo "Déploiement de l'application de test..."
    - mkdir -p ~/.kube
    - scp -o StrictHostKeyChecking=no hocine@$K8S_MASTER_IP:~/.kube/config ~/.kube/config
    - kubectl apply -f test-deployment.yaml
    - sleep 60
    - kubectl get pods -l app=nginx-test
    - kubectl get services nginx-test-service
    - echo "Application nginx accessible sur http://$K8S_MASTER_IP:30080"
    - echo "Dashboard Kubernetes accessible sur https://$K8S_MASTER_IP:30443"
  dependencies:
    - deploy_dashboard
  only:
    - main

# Job de nettoyage manuel
cleanup_cluster:
  stage: maintenance
  <<: *ansible_job
  script:
    - echo "Nettoyage du cluster..."
    - ansible-playbook -i inventory.yml cleanup-playbook.yml -v
  when: manual
  allow_failure: true
  only:
    - main
    - develop